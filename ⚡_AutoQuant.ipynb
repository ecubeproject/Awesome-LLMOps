{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecubeproject/Awesome-LLMOps/blob/main/%E2%9A%A1_AutoQuant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fD24jJxq7t3k"
      },
      "outputs": [],
      "source": [
        "# @title # ‚ö° AutoQuant\n",
        "\n",
        "# @markdown > üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "# @markdown ‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "\n",
        "# @markdown **Usage:** Download the model by **running this cell** and then run the cells corresponding to your quantization methods of interest.\n",
        "\n",
        "# @markdown To quantize a 7B or 8B model, GGUF only needs a T4 GPU, while the other methods require an L4 or A100 GPU.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## ü§ó Download model (required)\n",
        "# @markdown `MODEL_ID` is the ID of the model to quantize on the Hugging Face hub.\n",
        "MODEL_ID = \"mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown `USERNAME` is your username on Hugging Face.\n",
        "USERNAME = \"mlabonne\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown `HF_TOKEN` corresponds to the name of the secret that stores your [Hugging Face access token](https://huggingface.co/settings/tokens) in Colab.\n",
        "HF_TOKEN = \"HF_TOKEN\" # @param {type:\"string\"}\n",
        "\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "\n",
        "!pip install -qqq huggingface_hub --progress-bar off\n",
        "!pip install -qqq -U numpy==1.23.5 transformers --progress-bar off\n",
        "\n",
        "from huggingface_hub import create_repo, HfApi, ModelCard, snapshot_download\n",
        "from google.colab import userdata, runtime\n",
        "import shutil\n",
        "import fnmatch\n",
        "import os\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "hf_token = userdata.get(HF_TOKEN)\n",
        "api = HfApi()\n",
        "\n",
        "# Download model using huggingface_hub\n",
        "model_path = snapshot_download(\n",
        "    repo_id=MODEL_ID,\n",
        "    token=hf_token,\n",
        "    ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.onnx\"],  # Ignore certain file types\n",
        "    local_dir=MODEL_NAME\n",
        ")\n",
        "print(f\"Model downloaded to: {model_path}\")\n",
        "\n",
        "def upload_quant(base_model_id, quantized_model_name, quantization_type, save_folder, allow_patterns=None, bpw=None):\n",
        "    \"\"\"\n",
        "    Create a model card (if necessary), upload the quantized model to Hugging Face.\n",
        "\n",
        "    :param base_model_id: The ID of the base model\n",
        "    :param quantized_model_name: The name for the quantized model\n",
        "    :param quantization_type: The type of quantization (e.g., 'gguf', 'gptq', 'awq', 'hqq', 'exl2')\n",
        "    :param save_folder: The folder where the quantized model is saved\n",
        "    :param allow_patterns: List of file patterns to upload (default is None, which uploads all files)\n",
        "    :param bpw: Bits per weight (used for EXL2 quantization)\n",
        "    \"\"\"\n",
        "    # Initialize Hugging Face API\n",
        "    api = HfApi()\n",
        "\n",
        "    # Define the repository ID for the quantized model\n",
        "    if quantization_type == 'exl2':\n",
        "        repo_id = f\"{USERNAME}/{quantized_model_name}-{bpw:.1f}bpw-exl2\"\n",
        "    else:\n",
        "        repo_id = f\"{USERNAME}/{quantized_model_name}\"\n",
        "\n",
        "    # Try to load existing model card\n",
        "    try:\n",
        "        existing_card = ModelCard.load(repo_id)\n",
        "        print(f\"Model card already exists for {repo_id}. Skipping model card creation.\")\n",
        "    except Exception:\n",
        "        # If the model card doesn't exist, create a new one\n",
        "        card = ModelCard.load(base_model_id)\n",
        "        card.data.tags = [] if card.data.tags is None else card.data.tags\n",
        "        card.data.tags.append(\"autoquant\")\n",
        "        card.data.tags.append(quantization_type)\n",
        "        card.save(f'{save_folder}/README.md')\n",
        "        print(f\"Created new model card for {repo_id}\")\n",
        "\n",
        "    # Create or update the repository\n",
        "    create_repo(\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "        exist_ok=True,\n",
        "        token=hf_token\n",
        "    )\n",
        "\n",
        "    # Upload the model\n",
        "    api.upload_folder(\n",
        "        folder_path=save_folder,\n",
        "        repo_id=repo_id,\n",
        "        allow_patterns=allow_patterns,\n",
        "        token=hf_token\n",
        "    )\n",
        "\n",
        "    print(f\"Uploaded quantized model to {repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQ9gqCcXvkB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL0yGhbe3EFk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ## üß© GGUF\n",
        "\n",
        "# @markdown Recommended methods: `q2_k`, `q3_k_m`, `q4_k_m`, `q5_k_m`, `q6_k`, `q8_0`\n",
        "\n",
        "# @markdown Learn more about GGUF and quantization methods in [this article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html).\n",
        "\n",
        "QUANTIZATION_FORMAT = \"q2_k, q3_k_m, q4_k_m, q5_k_m, q6_k, q8_0\" # @param {type:\"string\"}\n",
        "QUANTIZATION_METHODS = QUANTIZATION_FORMAT.replace(\" \", \"\").split(\",\")\n",
        "gguf_repo_id = f\"{USERNAME}/{MODEL_NAME}-GGUF\"\n",
        "\n",
        "# # Install llama.cpp\n",
        "if not os.path.exists(\"llama.cpp\"):\n",
        "    !git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp && make\n",
        "    !pip install -r llama.cpp/requirements.txt\n",
        "\n",
        "# Convert to BF16\n",
        "out = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.bf16.gguf\"\n",
        "if os.path.exists(out):\n",
        "    print(f\"File {out} already exists. Skipping conversion.\")\n",
        "else:\n",
        "    !python llama.cpp/convert_hf_to_gguf.py {MODEL_NAME} --outtype bf16 --outfile {out}\n",
        "\n",
        "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/llama-quantize {out} {qtype} {method}\n",
        "\n",
        "    # Upload quant\n",
        "    upload_quant(\n",
        "        base_model_id=MODEL_ID,\n",
        "        quantized_model_name=f\"{MODEL_NAME}-GGUF\",\n",
        "        quantization_type=\"gguf\",\n",
        "        save_folder=MODEL_NAME,\n",
        "        allow_patterns=[\"*.gguf\", \"*.md\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE_R3AXG5Y-F",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ## üß† GPTQ\n",
        "\n",
        "# @markdown Learn more about the GPTQ algorithm in [this article](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html).\n",
        "\n",
        "!pip install auto-gptq optimum accelerate\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
        "\n",
        "BITS = 4 # @param {type:\"integer\"}\n",
        "GROUP_SIZE = 128 # @param {type:\"integer\"}\n",
        "DAMP_PERCENT = 0.1 # @param {type:\"number\"}\n",
        "\n",
        "# Quantize model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "quantization_config = GPTQConfig(bits=BITS, dataset=\"c4\", tokenizer=tokenizer, damp_percent=DAMP_PERCENT)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", quantization_config=quantization_config, low_cpu_mem_usage=True)\n",
        "\n",
        "# Save model and tokenizer\n",
        "save_folder = MODEL_ID + \"-GPTQ\"\n",
        "model.save_pretrained(save_folder, use_safetensors=True)\n",
        "tokenizer.save_pretrained(save_folder)\n",
        "\n",
        "# Upload quant\n",
        "upload_model(\n",
        "    base_model_id=MODEL_ID,\n",
        "    quantized_model_name=f\"{MODEL_NAME}-GPTQ\",\n",
        "    quantization_type=\"gptq\",\n",
        "    save_folder=save_folder\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZC9Nsr9u5WhN"
      },
      "outputs": [],
      "source": [
        "# @title # ü¶ô ExLlamaV2\n",
        "\n",
        "# @markdown Learn more about ExLlamaV2 in [this article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html).\n",
        "\n",
        "BPW = 4.5 # @param {type:\"number\"}\n",
        "\n",
        "# Install ExLLamaV2\n",
        "!git clone https://github.com/turboderp/exllamav2\n",
        "!pip install -e exllamav2\n",
        "!cp -r {MODEL_NAME} base_model\n",
        "!rm base_model/*.bin\n",
        "\n",
        "# Download dataset\n",
        "!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n",
        "\n",
        "# Quantize model\n",
        "save_folder = MODEL_ID + \"-EXL2\"\n",
        "!mkdir {save_folder}\n",
        "!python exllamav2/convert.py \\\n",
        "    -i base_model \\\n",
        "    -o {save_folder} \\\n",
        "    -c wikitext-test.parquet \\\n",
        "    -b {BPW}\n",
        "\n",
        "# Copy files\n",
        "!rm -rf quant/out_tensor\n",
        "!rsync -av --exclude='*.safetensors' --exclude='.*' base_model {save_folder}\n",
        "\n",
        "# Upload quant\n",
        "upload_quant(\n",
        "    base_model_id=MODEL_ID,\n",
        "    quantized_model_name=MODEL_NAME,\n",
        "    quantization_type=\"exl2\",\n",
        "    save_folder=save_folder,\n",
        "    bpw=BPW\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MyyUO2Fj3WHt"
      },
      "outputs": [],
      "source": [
        "# @title ## ‚öñÔ∏è AWQ\n",
        "\n",
        "# @markdown See the [AutoAWQ repository](https://github.com/casper-hansen/AutoAWQ) for more information.\n",
        "\n",
        "# Install AutoAWQ\n",
        "!pip install -qqq -U https://github.com/casper-hansen/AutoAWQ/releases/download/v0.2.4/autoawq-0.2.4+cu118-cp310-cp310-linux_x86_64.whl\n",
        "!pip install zstandard\n",
        "\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "BITS = 4 # @param {type: \"integer\"}\n",
        "GROUP_SIZE = 128 # @param {type: \"integer\"}\n",
        "VERSION = \"GEMM\" # @param {type: \"string\"}\n",
        "ZERO_POINT = True # @param {type: \"boolean\"}\n",
        "\n",
        "quant_config = {\n",
        "    \"w_bit\": BITS,\n",
        "    \"q_group_size\": GROUP_SIZE,\n",
        "    \"version\": VERSION,\n",
        "    \"zero_point\": ZERO_POINT\n",
        "}\n",
        "save_folder = MODEL_NAME + \"-AWQ\"\n",
        "\n",
        "# Quantize model\n",
        "model = AutoAWQForCausalLM.from_pretrained(MODEL_NAME, safetensors=True, low_cpu_mem_usage=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model.quantize(tokenizer, quant_config=quant_config)\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_quantized(save_folder)\n",
        "tokenizer.save_pretrained(save_folder)\n",
        "\n",
        "# Upload quant\n",
        "upload_quant(\n",
        "    base_model_id=MODEL_ID,\n",
        "    quantized_model_name=f\"{MODEL_NAME}-AWQ\",\n",
        "    quantization_type=\"awq\",\n",
        "    save_folder=save_folder\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iEhLsUjcnNR7"
      },
      "outputs": [],
      "source": [
        "# @title ## üêò HQQ\n",
        "\n",
        "# @markdown See the official [HQQ repository](https://github.com/mobiusml/hqq) for more information.\n",
        "\n",
        "# !git clone https://github.com/mobiusml/hqq.git\n",
        "# !pip install -e hqq\n",
        "# !python hqq/kernels/setup_cuda.py install\n",
        "# !pip install flash-attn --no-build-isolation\n",
        "# !pip install transformers --upgrade\n",
        "# !num_threads=8; OMP_NUM_THREADS=$num_threads CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "import torch\n",
        "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "\n",
        "BITS = 2 # @param {type:\"integer\"}\n",
        "GROUP_SIZE = 128 # @param {type:\"integer\"}\n",
        "\n",
        "# Quant config\n",
        "quant_config = BaseQuantizeConfig(\n",
        "    nbits=BITS,\n",
        "    group_size=GROUP_SIZE\n",
        ")\n",
        "\n",
        "# Quantize model\n",
        "model = HQQModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    cache_dir=\".\",\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model.quantize_model(quant_config=quant_config, device='cuda')\n",
        "\n",
        "# Save model and tokenizer\n",
        "save_folder = MODEL_ID + \"-HQQ\"\n",
        "model.save_quantized(save_folder)\n",
        "tokenizer.save_pretrained(save_folder)\n",
        "\n",
        "# Upload quant\n",
        "upload_quant(\n",
        "    base_model_id=MODEL_ID,\n",
        "    quantized_model_name=f\"{MODEL_NAME}-{BITS}bit-HQQ\",\n",
        "    quantization_type=\"hqq\",\n",
        "    save_folder=save_folder\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}